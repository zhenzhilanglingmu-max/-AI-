## CNNにおける Flatten と 全結合層の役割

### Flatten（平坦化）
- **役割**：多次元（2D, 3D）の特徴マップを 1次元ベクトルに並べ直す処理  
- **パラメータ**：なし（学習はしない、単なる形変換）
- **例**：7 × 7 × 512 → Flatten → 25,088次元ベクトル  

---

### 全結合層（Fully Connected Layer, Dense Layer）
- **役割**：Flattenされた 1次元ベクトルを **出力ノード数の次元を持つベクトル** に変換する  
  - 数式： y = Wx + b  
- **パラメータ**：大量にある（入力次元 × 出力次元 + 出力次元）  
- **まとめ役**：特徴を「タスクに必要な出力次元」に圧縮・変換する処理  
- **言い換え**：「出力ノード数の次元を持つベクトルに変換する処理」＝まとめていること  

---

### Flatten と 全結合層の違い
| 項目 | Flatten | 全結合層 |
|------|---------|---------|
| 役割 | 多次元を1次元に変換 | 1次元ベクトルを出力次元に変換 |
| パラメータ | なし | あり（Wx+bを学習） |
| 処理内容 | 並べ直し（reshape） | 重み付き和＋バイアス |
| 学習対象 | × | ○ |

---

### 注意点
- **全結合層は入力サイズに依存する**  
  - 入力画像が大きいほど Flatten後のベクトルも大きくなる  
  - → パラメータ数が爆発して過学習や計算コスト増の原因になる  

---

### FCN（Fully Convolutional Network）
- **全結合層を持たないCNN**  
- 代わりに **畳み込み層＋プーリング層** や **Global Average Pooling (GAP)** を利用  グローバルアベレージプーリング
- 入力サイズに依存しにくく、パラメータ数削減にもつながる  
- 主に **セグメンテーション**などで利用される  

---

### CNNの進化（まとめ）
- **古典的CNN（LeNet, AlexNet, VGG）**：Flatten → 全結合層 → 出力層  
- **モダンCNN（ResNet, Inception, EfficientNetなど）**：Flattenを省略し、**Global Average Pooling → 出力層**  

---
　1次元ベクトルを出力次元に変換　ここが全結合層のキモ　
