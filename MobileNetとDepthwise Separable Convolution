# 🤖 MobileNetとDepthwise Separable Convolution  
＋ 標準化（Standardization）とバッチ正規化（Batch Normalization）まとめ

---

## 📱 MobileNetとは？
- **軽量で高速なCNN（畳み込みニューラルネット）モデル**
- スマホや組み込みデバイスなど、**計算リソースが限られた環境向け**
- 特徴：
  - 高速で省メモリ
  - 精度をそこそこ保ちながらモデルを軽くできる
  - “頭ええ省エネくん”タイプのネットワーク

---

## ⚙️ Depthwise Separable Convolutionの仕組み  
（デプスワイズ・セパラブル・コンボリューション）

### 🔹 通常の畳み込みとの違い
通常の畳み込み（Convolution）は「**全チャンネルをまとめて一気に処理**」。  
Depthwise Separable Convは、それを「**2つの工程に小分け**」して軽量化する。

---

### 🧩 Step 1：Depthwise Convolution
- 各チャンネル（例：R・G・B）ごとに**独立して畳み込み**
- → 「ピンポイントで特徴を処理」する工程
- 各チャンネル単位で計算するため、計算量が激減！

### 🧩 Step 2：Pointwise Convolution（1×1 畳み込み）
- 各チャンネルで得られた特徴を**線形結合して統合**
- → 「まとめて意味を作る」工程
- チャンネル間の関係をここで再構築する。

---

### 💡 計算量の比較

| 層の種類 | 計算量（概算） |
|:--|:--|
| 通常の畳み込み | K² × M × N × H × W |
| Depthwise Separable Conv | K² × M × H × W + M × N × H × W |

結果：**約1/8〜1/9の計算量削減！**  
→ 「小分けにしたおかげで、1回の処理が楽になった」構造やね。

---

## 🔁 MobileNetではこのブロックを何回も積む
- 1ブロック（Depthwise + Pointwise）を**何層も積み重ねる**
- 各層が軽いため、全体としても高速＆省エネ  
- 「ピンポイント処理を繰り返して、抽象的な特徴を学習」する構造

---

## 🧠 MobileNetの主なバージョン

| バージョン | 特徴 |
|:--|:--|
| **MobileNetV1** | Depthwise Separable Convを初採用。軽量化の原点。 |
| **MobileNetV2** | Inverted Residual + Linear Bottleneckで効率UP。 |
| **MobileNetV3** | NAS（自動設計）＋ SEブロック（注意機構）導入。スマホ最適化。 |

---

## 🧮 標準化（Standardization）

### 🧠 概要
- **目的**：データのスケールをそろえて学習を安定させる。
- **タイミング**：学習前（前処理段階）
- **式**： x' = (x - μ) / σ
- μ（ミュー）：平均  
- σ（シグマ）：標準偏差
- **効果**：特徴量間のスケール差をなくす（例：cmやkgなどの単位差を統一）

---

## ⚙️ バッチ正規化（Batch Normalization）

### 🧠 概要
- **目的**：学習中の勾配爆発・消失を防ぎ、**学習を安定化・高速化**する。  
- **タイミング**：学習中（各層で適用）  
- **式**：x̂ = (x - μ_batch) / σ_batch
        y = γ * x̂ + β
- μ_batch：ミニバッチ内の平均  
- σ_batch：ミニバッチ内の標準偏差  
- γ（ガンマ）：スケール調整  
- β（ベータ）：平行移動  
- **効果**：出力分布を正規化し、**内部共変量シフト**を抑制。

---

## 🔍 標準化とBatchNormの比較

| 項目 | 標準化 | バッチ正規化 |
|:--|:--|:--|
| タイミング | 学習前（前処理） | 学習中（各層） |
| 対象 | 入力データ全体 | ミニバッチ単位の中間出力 |
| パラメータ | なし | γ・β（学習可能） |
| 目的 | スケール統一 | 学習安定化・高速化 |

---

## 💡 共通する発想：「分けて整える」

| 技術 | 分け方 | 目的 |
|:--|:--|:--|
| **Depthwise Separable Conv** | チャンネルごとに分けて処理 | 計算量削減（構造の効率化） |
| **Batch Normalization** | ミニバッチごとに分けて正規化 | 学習の安定化（数値の安定） |

どちらも共通して、
> 「全体を一気に処理せず、小分けにして整える」  
という設計思想を持っている。

---

## 🎯 一言でまとめると

> **MobileNet** は「構造を小分けにして速く」、  
> **BatchNorm** は「数値を整えて安定的に」。  
>  
> 両方に共通するのは「**分けて賢く処理する**」というAIの知恵や！

---

**タグ:**  
`#G検定 #ディープラーニング #MobileNet #DepthwiseSeparableConvolution #標準化 #BatchNormalization #軽量モデル #学習安定化 #GitHub学習日記`





















































