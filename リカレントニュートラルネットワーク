＃リカレントニュートラルネットワーク
＃RNN
＃時系列データ
＃リカレント（繰り返すこと）
＃クロスエントロピー損失
＃ＬＳＴＭ（ロングショートタームメモリー）
＃ＧＲＵ

ＲＮＮ（リカレントニュートラルネットワーク）
過去の出来事を次に活かす構造の教師あり学習モデル

少し詳しくすると（前の時刻で得た情報（隠れ状態ｈｔー１）を次の
時刻計算に持ち込む構造
過去の出来事を次の判断材料としていかせる

隠れ層は短期的な記憶
今しゃべってる会話とかに使われたり最終予測にも使われる

例えば、文章で（私は、スーパーマンに）ときたら
次の文章は（なりたいか）（なりたくない）
になりやすい

私はあなたのことが（好きか）（嫌い）
りんごを（食べるか）（食べないか）

ただの普通のＲＮＮやと（記憶力）が短いから昔すぎる情報は、薄れてしまう（これが勾配消失問題）や
この対策は次に話すＬＳＴＭとＧＲＵ

ＬＳＴＭは、三つのゲート構造持ってる時系列モデル
入力ゲート、忘却ゲート、出力ゲート
そして、セル状態（ＬＳＴＭだけが持ってる長期記憶に通路）
忘却ゲート　入力ゲートで何を覚えて何を忘れるかを決めて
重要なことだけを残す
直線的に情報を運ぶから勾配消失しにくいのがＬＳＴＭ

セル状態は長期記憶
人生のノート的な感じ

ＧＲＵは、ＬＳＴＭを簡略化したもの
二つのゲートだけ
リセットゲートと更新ゲート時系列データを処理する

ＲＮＮ相棒の損失関数はクロスエントロピー損失で確率のズレを修正
平均二乗誤差（ＭＳＥ）で数値のズレを修正

この二つで主にＲＮＮの予測と正解のズレを修正する

そして、そのズレ（損失）を小さくする方向に重みを更新する（逆伝播）

この流れを更新後にもう一度予測してズレをまた測る

この一連の流れを繰り返して精度を上げる

ことをしてるのがＲＮＮ


