＃特徴量エンジニアリング
＃エンコーディング
＃

特徴量エンジニアリングは、やることが多い「特徴を機械学習モデルが学習しやすいように、整理すること」
・特徴量エンジニアリングのキモは、カテゴリデータのエンコーディング

中身の説明していこ、多いでほんまに

１
数値系
・標準化（standordizatino)
平均を「０」、分散を「１」に揃える、線形モデルやＳＶＭで重要

・正規化（ｎｏｒｍａｌｉｚａｔｉｎｏ）
０～１に収める、ニューラルネットとか距離ベースで有効

・対数変換
外れ値がデカすぎる時に効く（例えば、売上金額）

１０００万を「７」に変換すること
桁を圧縮すること「スケール圧縮」
LOG１０（常用対数を使うこと）

・１０⇒１　１００⇒２　１０００⇒３　１００００⇒４　１０００００００⇒７
「０」の数を数えてる

２
カテゴリ系ここが重要！

・ラベルエンコーディング
文字を数字に変えるだけ　（例　赤色⇒１）

・ワンホットエンコーディング
カテゴリごとに列を作ってフラグ

例え、（赤チーム）　（青チーム）　（緑チーム）って分類してる
　カテゴリごとに列を作って「分類」してるって感じい

それぞれの列に一人ずつ
　　赤　青　緑
赤　１　０　０　一人
青　０　１　０　一人
緑　０　０　１　一人　　　　このくらい少ないカテゴリ分けやったら使える

・ターゲットエンコーディング
カテゴリを目的変数の平均で置換　強力やけど、リークに注意

・目的変数が買った＝１　買わない＝０という二値
（０/１）分類やとして

・あるカテゴリ（例　学生　社会人　主婦）の中で
「買った」の割合（＝平均値）を計算する
職業ごとのAiphone購入データがあったとすると

職業　　　　　　購入（買った）＝１　　　　　　買わない＝０
学生　　　　　　　１
学生　　　　　　　０
学生　　　　　　　０
社会人　　　　　　１
社会人　　　　　　１
主婦　　　　　　　０
主婦　　　　　　　１

平均すると、学生（１＋０＋０）/３＝０．３３
　　　　　　社会人（１＋１）/２＝１．０
　　　　　　主婦　（０＋１）/２＝０．５

この平均が、ターゲットエンコーディング後の値

・学習データだけで平均を計算しないとダメ
・テストデータも混ざってしまったらデータリークしてしまって、正確なデータができない

・頻度（カウント）エンコーディング

・カテゴリの出現回数で置換、カテゴリ数が多いときに便利
カテゴリに分けて、そのカテゴリが出てきた回数をカウントするだけ

・学生３００回　社会人１２００回　主婦４００回

・エンベティング＝「言葉を数字に翻訳する」こと
カテゴリデータを低次元の数値ベクトルに変換して、意味や関係性を持たせられること
例えていくと

・人間同士は「日本語」とか「英語」で話す
・でも、コンピューターには通じない・コンピューター＝数字の並び（ベクトル）
・似ている言葉は数字も近い　だから「りんご」⇒（１．２、１．３、１．１）「みかん」
・違う言葉は、数字も遠い　「りんご」と「飛行機」やと⇒「１．３、５．６、１２）みたいに、全然違う数字になって離れる

・コンピューターが「意味」をわかるようになる魔法の仕組みが「エンべティング」

３　時系列、日付
・ラグ特徴、一日前、七日前、など過去の値
・移動平均/移動分散、直近の傾向を入れる
・周期特徴、曜日、時間をｓｉｎ／ｃｏｓに変換（周期性を表現）
・日付分解、年、月、曜日、年末フラグ

４　集約特徴
・ユーザー単位：平均購入額・直近購入までの日数
・商品単位：販売回数、平均価格
・集団ごとの統計値を特徴として付加え

５　非構造データ
・テキスト　ＴＦ-ＩＤＦ、ＢａｇｏｆＷｏｒｄｓ　Ｗｏｒｄ２Ｖｅｃ
ＢＥＲＴ埋め込み

・画像
・ＣＮＮの中間特徴ベクトル

・音声　
・ＭＦＣＣ、スペクトル特徴

・特徴量エンジニアリングの注意

・データリークに注意、標準化やターゲットエンコーディングは、必ず「学習データだけ」計算
・過学習リスク、複雑な特徴を作りすぎると汎化性能が落ちる
・特徴選択、不要な特徴は削る（相関分析、正則化、ツリー重要度）


まとめると
「特徴量エンジニアリングは、データをモデルが理解できる形に整理して、学習に効く情報を引き出す技術」




















































　　　　　　　　　


















































