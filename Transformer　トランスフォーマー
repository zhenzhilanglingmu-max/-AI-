＃Transformer（トランスフォーマー
＃エンコーダー
＃デコーダー
＃attention機構（アテンション）

エンコーダーは、理解する人　デコーダーは、説明する人
アテンションattention機構は注意を向ける

・エンコーダー（理解する人）
・役割：入力（文章、画像など）を圧縮して理解
・アテンション：双方向自己注意（self-Attention）　前後の文脈を同時に見て、関係性を把握
　　　　　　　　　　　　使われる代表モデルＢＥＲＴ（読解特化）

　　　　　　　　　　　例えると、本を読んでノートにまとめる優等生

・デコーダー（説明する人）

・役割：頭の中の情報から、順に言葉を生成
・アテンション、自己注意（Causal Masked self Attention）　未来は見ないで左から右へ一語ずつ生成
・クロス注意（Cross-Attention）　エンコーダーの情報を参照してしゃべる
　　　　　　　　　　　　　　使われてる代表モデルはＧＰＴ（生成特化）

　　　　　　　　　　　　　　　　　例えると、ノートやカンペを見ながら（わかりやすく説明する人）

・エンコーダー＋デコーダー　合体は、＝先生
流れ
１　エンコーダー⇒文を理解し要点に圧縮
２　デコーダー⇒その要点を見ながら解説文を展開

代表モデル　　T５　ＢＡＲＴ　旧Ｓｅｑ２Ｓｅｑ翻訳モデル
・使い道翻訳、要約、言い換え

例え　知識を調べて理解してから生徒に教える先生

・トランスフォーマー（Transformer）
中身は、エンコーダー、やデコーダーを組み合わせた大枠の設計図（アーキテクチャ）

・特徴：アテンション機構（Attention is All you need）論文由来
・並列処理に強い⇒大規模学習が可能

・モジュール感覚で「エンコーダー」「デコーダー」「合体型」を作れる
＊モジュール感覚とは、用途に応じて、組み合わせを選べる柔軟性

代表モデル　・エンコーダー型、ＢＥＲＴ（検索、分類）
　　　　　　・デコーダー型、ＧＰＴ（生成、会話）
　　　　　　・合体型Ｔ５、ＢＥＲＴ（翻訳、要約）

そして、トランスフォーマーのキモの機構　アテンション機構中身が

１クエリ（Query）
・「今、見たい情報は何？」という質問文、みたいなベクトル
・例、翻訳で「apple」を処理中なら⇒今の「apple」に関連のある単語は、どれや？」ってゆう問いかけ

２キー（key）
・「この情報は、どんな特徴を持ってるか？」というレベルベクトル
・例、「fruit」フルーツ、「red」、「computer」など　それぞれの単語の特徴がキーになる

３バリュー（Valur）
・「その情報の中身（実際の意味の内容）」を持ってるベクトル
・例、「fruit＝果物の意味のベクトル」や「computer＝機械の意味のベクトル」

このタスクどこに関係あるー？ってクエリが質問して
キーがこれは、果物に関することやなーって見つけて
バリューでその知識に関することを調べて、答える仕組みやな

働き方、
１　ｑｕｅｒｙとｋｅｙの内積をとって関連度を出す
・「今の単語と、ほかの単語どれくらい関連ある？」をスコア化

２　SoftMaxで正規化して、確率にする
・「この単語に０％注目する」と決まる

３　Valueと掛け合わせて合計する
・関係のある単語の情報が集まってくる

１，２，３，で「文脈を理解した表現」が作れるわけ

クエリ＝質問/キー＝どの分野か/バリュー＝その分野の内容で動く　これがattention機構

少し深掘りで

T５　（Text-to-text Transfer Transformre）　transferトランスファー（転移学習のこと、「移す」「引き継ぐ」意味）
・全部テキストにして、解く先生
・アーキテクチャは、　エンコーダー＋デコーダー型のトランスフォーマー

・どんなタスクでも、入力テキストを与えれば、出力テキストとして答えが返ってくる
　　翻訳、要約、言い換え、の「入力を加工して、別の文章にするタスク」が超得意

















































































