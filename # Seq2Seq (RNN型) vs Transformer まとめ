# Seq2Seq (RNN型) vs Transformer まとめ

## 処理方式の違い

| モデル | エンコード（入力処理） | デコード（出力処理） |
|--------|------------------------|----------------------|
| **Seq2Seq (RNN型)** | 逐次処理（1単語ずつ読む） | 逐次処理（1単語ずつ出す） |
| **Transformer** | 並列処理（Self-Attentionで全単語同時に処理） | 逐次処理（自己回帰で1単語ずつ出す） |

---

## 特徴まとめ
- **Seq2Seq (RNN型)**  
  - 入力も出力も逐次処理  
  - 長文になると情報が圧縮されすぎて、長距離依存に弱い  
  - 並列計算が難しく、学習が遅い  

- **Transformer**  
  - 入力は並列処理できる（高速＆効率的）  
  - 出力は逐次処理（可変長出力はSeq2Seqと同じ仕組み）  
  - 自己注意(Self-Attention)により長距離依存をつかみやすい  

---

## 一言キャッチコピー
- Seq2Seq → **「ぜんぶコツコツ逐次」**  
- Transformer → **「インプットは一気！アウトプットはコツコツ」**

---
