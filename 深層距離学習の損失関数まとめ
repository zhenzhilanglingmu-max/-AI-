# 🧠 深層距離学習の損失関数まとめ  
（Deep Metric Learning Loss Functions）

---

## 🎯 概要
深層距離学習（Deep Metric Learning）は、  
**「似ているものは近く」「異なるものは遠く」**に配置されるように  
特徴ベクトル（embedding）を学習する手法。

目的は「クラスを予測する」ことではなく、  
**データ間の距離構造を学ぶこと**。

---

## 🔹 コントラスト損失（Contrastive Loss）

### 🧩 基本アイデア
2つのサンプル（A, B）を入力して、  
「同じクラスなら距離を小さく」「違うクラスなら距離を大きく」学習する。

### 🧮 損失式（テキスト形式）
L = (1 - y) * D^2 + y * max(0, m - D)^2
- y = 0 → 同じクラス（距離Dを小さく）  
- y = 1 → 異なるクラス（距離Dをmより大きく）  
- m：margin（距離の余白）

### ✅ 特徴
- シンプルで実装が簡単  
- 絶対距離を基準に学習  
- 代表例：Siamese Network

### 📍 イメージ
> 「仲いい or 仲悪い」をはっきり分けるタイプ。

---

## 🔹 トリプレット損失（Triplet Loss）

### 🧩 基本アイデア
3つのサンプル（Anchor, Positive, Negative）を使用。  
AnchorとPositiveは同じクラス、Negativeは異なるクラス。

目的：  
**Anchor-Positive の距離 < Anchor-Negative の距離**  
となるように学習。

### 🧮 損失式
L = max(0, D(a, p) - D(a, n) + m)
- D(a, p)：アンカーとポジティブの距離  
- D(a, n)：アンカーとネガティブの距離  
- m：margin（余裕距離）

### ✅ 特徴
- 相対距離（順位関係）を学べる  
- Hard Negative Mining が重要  
- 代表例：FaceNet（Google）

### 📍 イメージ
> 「どっちがより近いか？」を学ぶタイプ。

---

## 🔸 コントラスト vs トリプレット 比較表

| 項目 | コントラスト損失 | トリプレット損失 |
|------|------------------|------------------|
| 学習単位 | ペア（2つ） | 3つ組（A,P,N） |
| 学習内容 | 絶対距離 | 相対距離 |
| タスク | 同 or 異 | より近い or より遠い |
| メリット | シンプル・直感的 | 強力な関係学習が可能 |
| デメリット | ペア数が多くなる | Hard Negative選びが難しい |
| 主な用途 | 類似度学習・特徴抽出 | 顔認識・画像検索など |

---

## 🌱 発展系（派生損失）

| 損失名 | ベース | 特徴 |
|---------|---------|------|
| N-pair Loss | トリプレット | 複数ネガティブを同時に考慮 |
| Proxy-NCA | トリプレット | クラス代表点（proxy）を使って効率化 |
| ArcFace / CosFace / SphereFace | コントラスト＋分類 | Softmaxに角度マージンを追加し高精度化 |

---

## 💬 まとめ一言
> 深層距離学習の世界は、  
> **コントラスト損失で始まり、トリプレット損失で完成する。**




