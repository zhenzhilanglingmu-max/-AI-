＃単純パーセプトロン
＃多層パーセプトロン


   単純パーセプトロン　将棋に例えると、飛車のような分け方をするモデル
　　・人工ニューロンの最初のモデル（１９５０年代に出たやつ）
　　　　・入力を重み付きで足して、しきい値を超えたら「１（オン）」　超えなっかたら「０（オフ）」
　　　　　　・線を引いて分類するマシンみたいに感じ

　　　　　　
　　　　　　　　　　　　　数式チャットGPTから貼り付け
　　　　　　　　　　　　　　出力 y = f(w1x1 + w2x2 + … + b)

　　　　　　　　　　　　　f はステップ関数（閾値でバチンと切る）

　　　　　　　　　　　　　　　結果は 0 か 1 の2値分類

　　　できること　　ＡＮＤ（両方１なら１）　ＯＲ（どっちか１なら１）
　　　　　　　　　点を直線で分けられるルールならいける　条件が単純ならできる

　　　できないこと　線形分離でできない問題は無理

　　　　　　ＸＯＲ（片方だけ１なら１）　条件が多いと分けられない

　　　　　　　チェッカーボードみたいに並ぶＸＯＲは交互に並ぶから線１本じゃ無理
　　　　　　　基本は　真っ直ぐ　縦か横


　　　　　　　　多層パーセプトロン（ＭＬＰ）　マルチレイヤーパーセプトロン　　将棋で例えると、飛車も角も金も使っていろんな条件を組み合わせる「チーム戦」ができる

　　　　　　　　　・隠れ層（Ｈｉｄｄｅｎ　Ｌａｙｅｒ）
　　　　　　　　　　　　　が入ることで、直線を組み合わせて曲がった境界や複雑なルールを表現できる

　　　　　　　　　　・構造　１入力層　特徴量（身長、体重、テストの点数など）を入れる

　　　　　　　　　　　　　　２　・隠れ層　複数のニューロンで「中間の特徴」を学習
　　　　　　　　　　　　　　　　・活性化関数（シグモイド、ＲｅＬＵ、など）で非線形変換＝まっすぐじゃない線
　　　　　　　　　　　　　　　　・これで「ぐにゃっとした境界」や「複雑な条件」を表現できる（分けれるってこと、例えば２０代男性、筋トレしてる、瞑想してる、身長１６２㎝）みたいな条件でしてる、してない

　　　　　　　　　　　　　３　出力層　分類結果（０／１）や確率、回帰値などを出す

　　　　　　　　　　　　　　学習の仕組み　・順伝播（Ｆｏｒｗａｒｄ　Ｐｒｏｐａｇａｔｉｏｎ）
　　　　　　　　　　　　　　　　　　　　　・誤差逆伝播（Ｂａｃｋ　Ｐｒｏｐａｇａｔｉｏｎ）バックプロパゲーション
　　　　　　　　　　　　　　　　　　　　　　　　　出力と正解のズレ（誤差）を計算して、重みをちょっとずつ修正する　これを繰り返して、精度を上げていく
　　　　　できること
　　　　　　　・ＸＯＲみたいな非線形な問題を解ける　・画像認識や音声認識の基礎になる　・万能近位の定理、隠れ層が１つでもニューロンを十分に増やせば「どんな関数でも近位できる」

　　　　できないこと　限界
　　　　　　　・隠れ層が増えると計算コストが爆増　・勾配消失問題（昔は、シグモイドばっかりで深くすると学習できなかった）　　
　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　・そこで、ＲｅＬＵとかドロップアウト、バッチ正規化みたいな工夫が発展してディープラーニングにつながった























































